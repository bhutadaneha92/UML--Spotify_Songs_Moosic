# -*- coding: utf-8 -*-
"""Function_Moosic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FiJzu2968CjaJJceRUU_0YPRUrT6avOo
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

# Import the model
from sklearn.cluster import KMeans as SKLearnKMeans

# To plot graph and visualize data
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go

# Compute the distance matrix
from sklearn.metrics import pairwise_distances, silhouette_score

# Transform features by scaling
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs

from sklearn import set_config
set_config(transform_output="pandas")

# To get analysis of entire data
#from ydata_profiling import ProfileReport

# For static plots
# %matplotlib inline
# Or:
#%matplotlib notebook  # For interactive plots

# !pip install ydata_profiling
# !pip install ipywidgets


def HeatmapPlotFunc(original_df, scaler_df, random_seed=42, random_samples=25):
    """
    Generates heatmaps for pairwise distances of the original and scaled datasets.

    Parameters:
    - original_df (pd.DataFrame): The original DataFrame.
    - scaler_df (pd.DataFrame): The scaled DataFrame (e.g., MinMax scaled).
    - random_seed (int): Random seed for reproducibility when sampling rows.
    - random_samples (int): Number of random samples for the heatmap.

    Returns:
    - None: Displays heatmaps of the sampled data.
    """
    # Calculate pairwise distances for the scaled DataFrame
    scaler_df_distances = pd.DataFrame(
        pairwise_distances(scaler_df),
        index=original_df.index,
        columns=original_df.index
    )

    # Calculate pairwise distances for the original DataFrame
    original_df_distances = pd.DataFrame(
        pairwise_distances(original_df),
        index=original_df.index,
        columns=original_df.index
    )

    # Randomly sample indices for smaller heatmaps
    sample_indices = original_df.sample(random_samples, random_state=random_seed).index
    scaler_df_sample_distances = scaler_df_distances.loc[sample_indices, sample_indices]
    original_df_sample_distances = original_df_distances.loc[sample_indices, sample_indices]

    # Create subplots for the heatmaps
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))  # Adjusted size for better visualization

    # Plot the heatmaps
    sns.heatmap(original_df_sample_distances, ax=ax1, cmap='coolwarm', linewidths=.2, annot=False)
    sns.heatmap(scaler_df_sample_distances, ax=ax2, cmap='coolwarm', linewidths=.2, annot=False)

    # Add titles
    ax1.set_title('Original Distances (Sampled)', fontsize=16)
    ax2.set_title('Scaled Distances (Sampled)', fontsize=16)

    # Display the heatmaps
    plt.tight_layout()
    plt.show()



def PCAFunc(scaler_df):
    """
    Perform PCA on a scaled DataFrame and visualize the explained variance.

    Parameters:
    - scaler_df (pd.DataFrame): Scaled DataFrame (e.g., from MinMaxScaler or StandardScaler).

    Returns:
    - pca_basic_df (np.ndarray): Transformed data after applying PCA.
    - explained_variance_array_df (pd.DataFrame): DataFrame containing variance explained by each component.
    """
    # Initialize the PCA object
    pca = PCA()

    # Fit the PCA object to the data
    pca.fit(scaler_df)

    # Transform the data using PCA
    pca_basic_df = pca.transform(scaler_df)

    # Get the variance explained by each principal component
    explained_variance_array = pca.explained_variance_ratio_

    return pca_basic_df, explained_variance_array


def InertiaElbowFunc(scaler_df, max_k_values=[20, 30, 50], random_seed=42): #if apply PCA then pass pca_variance_df
    """
    Generate elbow plots of inertia scores for various max_k values.

    Parameters:
    - pca_variance_df (pd.DataFrame): DataFrame with PCA-transformed features.
    - max_k_values (list): List of maximum cluster counts to evaluate.
    - random_seed (int): Seed for reproducibility in KMeans.

    Returns:
    - inertia_list_of_lists (list): List of inertia scores for each max_k range.
    """
    # Create a list to store the inertia scores for each max_k
    inertia_list_of_lists = []

    # Iterate over each max_k value
    for max_k in max_k_values:
        print(f"Processing max_k = {max_k}")

        # Temporary list to store inertia scores for current max_k
        inertia_scores = []

        # Calculate inertia scores for cluster numbers from 1 to max_k
        for i in range(1, max_k + 1):
            kmeans_model = SKLearnKMeans(n_clusters=i, n_init="auto", random_state=random_seed)
            kmeans_model.fit(scaler_df)
            inertia_scores.append(kmeans_model.inertia_)

        # Append the scores to the main list
        inertia_list_of_lists.append(inertia_scores)

    # Plot the results
    for idx, inertia_scores in enumerate(inertia_list_of_lists):
        max_k = max_k_values[idx]
        cluster_numbers = list(range(1, max_k + 1))  # Correct range for clusters

        # Create a line plot for inertia scores
        plot = sns.relplot(
            kind="line",
            x=cluster_numbers,
            y=inertia_scores,
            marker="o",
            height=5,
            aspect=1.5
        )

        # Customize the plot
        plot.set(
            title=f"Inertia Scores for 1 to {max_k} Clusters",
            xlabel="Number of Clusters",
            ylabel="Inertia"
        )

    # Display the plots
    plt.show()

    # Return the list of inertia scores
    return #inertia_list_of_lists



def SilhouetteScoreFunc(scaler_df, max_k_values=[20, 30, 50], random_seed=42):
    """
    Generate silhouette score plots for various max_k values.

    Parameters:
    - scaler_df (pd.DataFrame): Scaled DataFrame or PCA-transformed features.
    - max_k_values (list): List of maximum cluster numbers to evaluate.
    - random_seed (int): Seed for reproducibility in KMeans.

    Returns:
    - silhouette_list_of_lists (list): List of silhouette scores for each max_k range.
    """
    # Create a list to store the silhouette scores for each max_k
    silhouette_list_of_lists = []

    # Iterate over each max_k value
    for max_k in max_k_values:
        print(f"Processing max_k = {max_k}")

        # Temporary list to store silhouette scores for current max_k
        sil_scores = []

        # Calculate silhouette scores for cluster numbers from 2 to max_k
        for n_clusters in range(2, max_k + 1):
            kmeans_model = SKLearnKMeans(n_clusters=n_clusters, n_init="auto", random_state=random_seed)
            kmeans_model.fit(scaler_df)
            labels = kmeans_model.labels_

            # Calculate silhouette score
            score = silhouette_score(scaler_df, labels)
            sil_scores.append(score)

        # Append scores to the main list
        silhouette_list_of_lists.append(sil_scores)

    # Plot the results
    for idx, sil_scores in enumerate(silhouette_list_of_lists):
        max_k = max_k_values[idx]
        cluster_numbers = list(range(2, max_k + 1))  # Correct range for clusters

        # Create a line plot for silhouette scores
        plot = sns.relplot(
            kind="line",
            x=cluster_numbers,
            y=sil_scores,
            marker="o",
            height=5,
            aspect=1.5
        )

        # Customize the plot
        plot.set(
            title=f"Silhouette Scores for 2 to {max_k} Clusters",
            xlabel="Number of Clusters",
            ylabel="Silhouette Score"
        )

    # Display the plots
    plt.show()

    # Return the list of silhouette scores
    return #silhouette_list_of_lists


def ApplyKMeans(scaler_df, n_clusters=22, random_seed=42, **kwargs):
    """
    Apply KMeans clustering to the given DataFrame.

    Parameters:
    - scaler_df (pd.DataFrame): Scaled DataFrame to cluster.
    - n_clusters (int): Number of clusters for KMeans.
    - random_seed (int): Random seed for reproducibility.
    - **kwargs: Additional parameters for sklearn's KMeans.

    Returns:
    - clustered_df (pd.DataFrame): DataFrame with an added 'cluster' column.
    - kmeans_model (KMeans): Fitted KMeans model.
    """
    # Initialize the KMeans model
    kmeans_model = SKLearnKMeans(n_clusters=n_clusters, random_state=random_seed, **kwargs)

    # Fit the model to the data
    kmeans_model.fit(scaler_df)

    # Obtain the cluster labels
    clusters = kmeans_model.labels_

    # Attach the cluster output to a copy of the original DataFrame
    clustered_df = scaler_df.copy()
    clustered_df["cluster"] = clusters

    return clustered_df




def RadarPlotFunc(clustered_df):
  # Create an empty list to store the Scatterpolar object of each cluster
  scatter_objects = []

  # State the label for each arm of the chart
  categories = clustered_df.columns

  # Iterate over the unique clusters and add an object for each cluster to the list
  for cluster in sorted(clustered_df['cluster'].unique()):

    # Find the mean value for each column of the cluster
    cluster_means = [clustered_df.loc[clustered_df["cluster"] == cluster, categories[0]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[1]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[2]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[3]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[4]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[5]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[6]].mean(),
                    clustered_df.loc[clustered_df["cluster"] == cluster, categories[7]].mean()]


    # Create a Scatterpolar object for the cluster
    cluster_scatter = go.Scatterpolar(
      r = cluster_means, # set the radial coordinates
      theta = categories, # the names of the columns
      fill = 'toself', # fills in the space with colour
      name = f'Cluster {cluster}' # adds the name of the cluster
    )

    # Add the Scatterpolar object to the list
    scatter_objects.append(cluster_scatter)

  # Create the figure (the white area)
  fig = go.Figure()

  # Add the scatter objects to the figure
  fig.add_traces(scatter_objects)

  # Add extras to the plot, such as title
  fig.update_layout(
    title_text = 'Radar chart of mean songs preferences by cluster',
    height = 600,
    width = 800,
    polar = dict(
      radialaxis = dict(
        visible = True#, # visibility of the numbers on the arm
        #range = [0, 1] # scale of the plot
      )),
    showlegend = True
  )

  # Show the initialised plot and the trace objects
  fig.show()